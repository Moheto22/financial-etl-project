{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d7d839-d1ec-4c39-8dbc-716df924de2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **ETL NASDAQ and BTC daily data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de3c2214-8348-4218-af16-060d6f9ee387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Library importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e5709b4-c7ea-4f4b-b480-d9d03b1a055b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as fy\n",
    "import logging\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b618a14f-5090-41eb-9f17-de36c3170bfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Logging configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc03ae3d-efe8-4e64-9f4c-f49ba69ad172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    stream=sys.stdout\n",
    ")\n",
    "\n",
    "logger = logging.getLogger('financial_etl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0449902b-8623-498a-b4e4-0e51976cbdc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Init the dictionary of the status of the ETL execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cfc6d28-6aa3-4912-9f9e-46f4086aa093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "etl_results = {\n",
    "    'status': 'success',\n",
    "    'total_companies': 0, \n",
    "    'successful_downloads': 0,\n",
    "    'failed_companies': [],\n",
    "    'errors': [],\n",
    "    'data_quality_issues': [],\n",
    "    'fatal_error':'none',\n",
    "    'successful_registration':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5413a064-4a39-4df4-9dfa-f3189244c821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We get all the companys from AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ea5d48-cb23-4708-98e1-80a0105c8d42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    # I get the secret credentials to access to my AWS S3 bucket\n",
    "    access_key = dbutils.secrets.get(scope=\"aws-credentials\", key=\"access-key\")\n",
    "    secret_key = dbutils.secrets.get(scope=\"aws-credentials\", key=\"secret-key\")\n",
    "\n",
    "    spark.conf.set(\"fs.s3a.access.key\", access_key)\n",
    "    spark.conf.set(\"fs.s3a.secret.key\", secret_key)\n",
    "    # This csv i import here has got the list of all the companys i have to process\n",
    "    companies_df = spark.read.option(\"header\", \"true\").csv(\"s3a://data-lake-finan/config/lis-companys.csv\")\n",
    "\n",
    "    bitcoin_related_companies = [row.companys for row in companies_df.collect()]\n",
    "\n",
    "    etl_results['total_companies'] = len(bitcoin_related_companies)\n",
    "\n",
    "    logger.info(f'We will process {len(bitcoin_related_companies)} companys')\n",
    "except Exception as e:\n",
    "    etl_results['status'] = 'failed'\n",
    "    etl_results['fatal_error'] = str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e30f0050-de73-43c9-8295-49b06d612476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creation of the functions that are responsible for verifying the quality of the data and also for transforming it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169b5ebb-2a0c-4f7e-a0b5-43f0c9bf6d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def has_wrong_columns(df : pd.DataFrame,required_columns : list):\n",
    "    \n",
    "    columns_in_dataframe = list(df.columns)\n",
    "\n",
    "    missing_columns = False\n",
    "    # If i found one missing column i will end the loop and return True\n",
    "    i = 0\n",
    "    while(i<len(required_columns) and not missing_columns):\n",
    "        if required_columns[i] not in columns_in_dataframe:\n",
    "            missing_columns = True\n",
    "        i += 1\n",
    "    return missing_columns\n",
    "\n",
    "def check_has_NaN(df : pd.DataFrame):\n",
    "    contain_NaN = df.isna().any().any()\n",
    "    return contain_NaN\n",
    "    \n",
    "def modify_names(df : pd.DataFrame, company : str):\n",
    "    for col in df.columns:\n",
    "        if col != 'Date':\n",
    "            df.rename(columns = {col : f'{col}_{company}'}, inplace=True)\n",
    "    return df\n",
    "    \n",
    "def transformation_and_generation_new_variables(df_original : pd.DataFrame, company : str):\n",
    "    df = df_original.copy()\n",
    "    df['Range'] = df['High']-df['Low']\n",
    "    logger.info(f'The variable Range was generated for {company}')\n",
    "    df['Range_pct'] = ((df['Range']) / df['Open']) * 100\n",
    "    logger.info(f'The variable Range_pct was generated for {company}')\n",
    "    df['Variation_pct'] = ((df['Close']-df['Open'])/df['Open'])*100\n",
    "    logger.info(f'The variable Variation_pct was generated for {company}')\n",
    "    df[['Open','High','Low','Close','Range','Variation_pct','Range_pct']] = df[['Open','High','Low','Close','Range','Variation_pct','Range_pct']].astype('float32')\n",
    "    logger.info(f'The data of {company} was optimized parsing to float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9706d8be-6fa1-49c7-b28e-9e71cb0610c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Extraction of data from companies on the NASDAQ today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc3ca9b-59f2-422a-9501-3d479a2a9b4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "period = dbutils.widgets.get(\"period\")\n",
    "df_economic_data = pd.DataFrame()\n",
    "all_dataframes = [] #Here i will collect all the diferents dataframes of all the companys\n",
    "required_columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "if etl_results['status']!='failed':\n",
    "    for company in bitcoin_related_companies:\n",
    "        try:\n",
    "            data = fy.download(company,period=period,interval='1d',auto_adjust=True)\n",
    "            if len(data) == 0 :\n",
    "                logger.warning(f'The company {company} failed during the download')\n",
    "                etl_results['failed_companies'].append(company)\n",
    "                etl_results['errors'].append(f'{company}: failed during the download')\n",
    "            else :\n",
    "                logger.info(f'The info of the {company} its successfully downloaded')\n",
    "                etl_results['successful_downloads'] += 1\n",
    "                data.columns = data.columns.droplevel(1)\n",
    "                data = data.reset_index() #I clean the unnecessary index comes from yfinance\n",
    "                if has_wrong_columns(data,required_columns) == True:\n",
    "                    logger.warning(f'The data structure of the company {company} is wrong')\n",
    "                    etl_results['failed_companies'].append(company)\n",
    "                    etl_results['data_quality_issues'].append(f'The data of {company} has wrong structure')\n",
    "                else :\n",
    "                    logger.info(f'{company} data structure its correct')\n",
    "                    if check_has_NaN(data) == True:\n",
    "                        logger.warning(f'The data of {company} has NULL values')\n",
    "                        etl_results['failed_companies'].append(company)\n",
    "                        etl_results['data_quality_issues'].append(f'The data of {company} has NULL values')\n",
    "                    else : \n",
    "                        logger.info(f'{company} has not any NULL object')\n",
    "                        clean_data = data[required_columns]\n",
    "                        clean_data = transformation_and_generation_new_variables(clean_data,company)\n",
    "                        clean_data = modify_names(clean_data,company) # I change the name of the columns for when i merge the dataframes\n",
    "                        all_dataframes.append(clean_data)\n",
    "                        logger.info(f'The data of {company} its correctly cleaned, transformed and registered')\n",
    "                        etl_results['successful_registration'].append(company)\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error with {company}: {e}')\n",
    "            etl_results['failed_companies'].append(company)\n",
    "            etl_results['errors'].append(f'{company}: {str(e)}')\n",
    "\n",
    "    try:\n",
    "        if len(all_dataframes) == 0:\n",
    "            raise Exception('No data registered')\n",
    "        df_economic_data = all_dataframes[0] # I put the first one to initlize the column of Date where i will merge the dataframes\n",
    "        for df in all_dataframes[1:]:\n",
    "            df_economic_data = df_economic_data.merge(df,on='Date',how='outer')\n",
    "        logger.info(f'dataframe is correctyl unified, has {len(df_economic_data)} rows and {len(df_economic_data.columns)} columns')\n",
    "    except Exception as e: \n",
    "        etl_results['status'] = 'failed'\n",
    "        etl_results['fatal_error'] = str(e)\n",
    "        logger.error(f'A fatal error has ocurred during the data downloading {str(e)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71458425-4450-4a80-b7c8-463a71d902cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_economic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9c79c7-a682-414f-8a6f-8b7cff9efb59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data load to Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1491cfa3-0bae-43db-8e79-010c377c50ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if etl_results['status']!='failed':\n",
    "    try:\n",
    "        spark_df = spark.createDataFrame(df_economic_data)\n",
    "        spark_df.write.format(\"delta\").mode('append').option(\"mergeSchema\", \"true\").save(\"s3a://data-lake-finan/data/\") # I register the data in delta lake of AWS S3\n",
    "    except Exception as e:\n",
    "        etl_results['status'] = 'failed'\n",
    "        etl_results['fatal_error'] = str(e)\n",
    "\n",
    "dbutils.notebook.exit(json.dumps(etl_results)) # I send the results of the ETL for being captured by Airflow"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "financial_etl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
